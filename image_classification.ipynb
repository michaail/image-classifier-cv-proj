{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a11fc11",
   "metadata": {},
   "source": [
    "# Bike Category Classification: CNN Comparison Study\n",
    "\n",
    "This notebook compares **CNN architectures trained from scratch** vs **pretrained models fine-tuned** for bicycle category classification (5 classes: cargo, fold, hybrid, mtb, road).\n",
    "\n",
    "## Experiment Design (32 Experiments)\n",
    "- **2 Architectures**: MobileNetV2 (lightweight, suited for small datasets) & ResNet18 (skip connections)\n",
    "- **4 Data Scenarios**: E1 (all), E2 (synthetic), E3 (real), E4 (balanced)\n",
    "- **4 Augmentation Variants**: A1 (baseline), A2 (geometric), A3 (color), A4 (heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET DOWNLOAD & SETUP\n",
    "# =============================================================================\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_URL = \"https://github.com/michaail/bike-classifier-dataset/archive/refs/heads/main.zip\"\n",
    "DATASET_DIR = \"dataset\"\n",
    "SPLIT_DIR = \"dataset_split\"\n",
    "\n",
    "# CRITICAL: Fixed seed for reproducible, immutable test set\n",
    "SPLIT_SEED = 42\n",
    "\n",
    "def download_dataset(url=DATASET_URL, extract_dir=DATASET_DIR):\n",
    "    \"\"\"Downloads and extracts the dataset.\"\"\"\n",
    "    if os.path.exists(os.path.join(extract_dir, \"bike-classifier-dataset-main\")):\n",
    "        print(\"Dataset already downloaded\")\n",
    "        return\n",
    "    \n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    zip_path = \"dataset.zip\"\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"Dataset downloaded and extracted\")\n",
    "\n",
    "def create_deterministic_splits(source_base_path, target_base_path=SPLIT_DIR):\n",
    "    \"\"\"\n",
    "    Creates train/val/test splits with FIXED test set.\n",
    "    \n",
    "    Split sizes per class (20 images each):\n",
    "    - Train: 12 images\n",
    "    - Val: 4 images  \n",
    "    - Test: 4 images (FIXED - never changes between experiments)\n",
    "    \n",
    "    Total real test set: 4 √ó 5 classes = 20 images (as per requirements)\n",
    "    \"\"\"\n",
    "    \n",
    "    # CRITICAL: Always use same seed for identical test set\n",
    "    rng = random.Random(SPLIT_SEED)\n",
    "    \n",
    "    source_real = os.path.join(source_base_path, 'real')\n",
    "    source_syn = os.path.join(source_base_path, 'synthetic')\n",
    "    \n",
    "    # Split configuration: Train 12, Val 4, Test 4 = 20 total per class\n",
    "    SPLIT_COUNTS = {'train': 12, 'val': 4, 'test': 4}\n",
    "    \n",
    "    # Clean previous splits\n",
    "    if os.path.exists(target_base_path):\n",
    "        shutil.rmtree(target_base_path)\n",
    "    \n",
    "    classes = [d for d in os.listdir(source_real) if os.path.isdir(os.path.join(source_real, d))]\n",
    "    print(f\"Classes detected: {classes}\")\n",
    "    \n",
    "    def copy_files(file_list, subset_name, split_name, class_name):\n",
    "        dest_dir = os.path.join(target_base_path, subset_name, split_name, class_name)\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        for file_path in file_list:\n",
    "            shutil.copy2(file_path, dest_dir)\n",
    "    \n",
    "    for class_name in classes:\n",
    "        real_imgs = glob(os.path.join(source_real, class_name, '*.*'))\n",
    "        syn_imgs = glob(os.path.join(source_syn, class_name, '*.*'))\n",
    "        \n",
    "        # Sort first for determinism, then shuffle with fixed seed\n",
    "        real_imgs.sort()\n",
    "        syn_imgs.sort()\n",
    "        rng.shuffle(real_imgs)\n",
    "        rng.shuffle(syn_imgs)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_end = SPLIT_COUNTS['train']\n",
    "        val_end = train_end + SPLIT_COUNTS['val']\n",
    "        test_end = val_end + SPLIT_COUNTS['test']\n",
    "        \n",
    "        # Real data splits\n",
    "        if len(real_imgs) >= test_end:\n",
    "            r_train = real_imgs[:train_end]\n",
    "            r_val = real_imgs[train_end:val_end]\n",
    "            r_test = real_imgs[val_end:test_end]\n",
    "            \n",
    "            copy_files(r_train, 'real', 'train', class_name)\n",
    "            copy_files(r_val, 'real', 'val', class_name)\n",
    "            copy_files(r_test, 'real', 'test', class_name)\n",
    "        \n",
    "        # Synthetic data splits\n",
    "        if len(syn_imgs) >= test_end:\n",
    "            s_train = syn_imgs[:train_end]\n",
    "            s_val = syn_imgs[train_end:val_end]\n",
    "            s_test = syn_imgs[val_end:test_end]\n",
    "            \n",
    "            copy_files(s_train, 'synthetic', 'train', class_name)\n",
    "            copy_files(s_val, 'synthetic', 'val', class_name)\n",
    "            copy_files(s_test, 'synthetic', 'test', class_name)\n",
    "        \n",
    "        # Full (combined) splits\n",
    "        if len(real_imgs) >= test_end or len(syn_imgs) >= test_end:\n",
    "            copy_files(r_train + s_train, 'full', 'train', class_name)\n",
    "            copy_files(r_val + s_val, 'full', 'val', class_name)\n",
    "            # Test set: USE ONLY REAL for consistent evaluation\n",
    "            copy_files(r_test, 'full', 'test', class_name)\n",
    "    \n",
    "    print(f\"Splits created at: {target_base_path}\")\n",
    "    return target_base_path\n",
    "\n",
    "# Download and create splits\n",
    "download_dataset()\n",
    "source_path = os.path.join(DATASET_DIR, \"bike-classifier-dataset-main\")\n",
    "create_deterministic_splits(source_path)\n",
    "\n",
    "# Verify splits\n",
    "print(\"\\nüìä Dataset Split Summary:\")\n",
    "for subset in ['real', 'synthetic', 'full']:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = os.path.join(SPLIT_DIR, subset, split)\n",
    "        if os.path.exists(path):\n",
    "            total = sum(len(os.listdir(os.path.join(path, c))) for c in os.listdir(path))\n",
    "            print(f\"  {subset}/{split}: {total} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets.folder as folder_module\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8178dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPRODUCIBILITY SEEDS\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd9a0",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "### Why these models?\n",
    "For a **small dataset (~50-100 training images)**, large models like VGG16 (138M params) will severely overfit. Instead, we use:\n",
    "\n",
    "1. **MobileNetV2** (~3.4M params): Designed for efficiency, uses inverted residuals and linear bottlenecks. Excellent for small datasets with pretrained weights.\n",
    "\n",
    "2. **ResNet18** (~11M params): Uses skip connections to enable deeper training. Good balance of capacity and regularization.\n",
    "\n",
    "Both architectures are compared:\n",
    "- **From Scratch**: Random initialization, trained only on our data\n",
    "- **Fine-tuned**: Pretrained on ImageNet, adapted to our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2765edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified wrapper for CNN models that returns (output, features).\n",
    "    Supports both scratch and pretrained initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_dim = feature_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # For models where we need to extract features\n",
    "        if hasattr(self.model, 'features'):\n",
    "            # MobileNetV2 style\n",
    "            features = self.model.features(x)\n",
    "            features = nn.functional.adaptive_avg_pool2d(features, 1)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            out = self.model.classifier(features)\n",
    "        else:\n",
    "            # ResNet style - extract features before fc\n",
    "            x = self.model.conv1(x)\n",
    "            x = self.model.bn1(x)\n",
    "            x = self.model.relu(x)\n",
    "            x = self.model.maxpool(x)\n",
    "            x = self.model.layer1(x)\n",
    "            x = self.model.layer2(x)\n",
    "            x = self.model.layer3(x)\n",
    "            x = self.model.layer4(x)\n",
    "            features = self.model.avgpool(x)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            out = self.model.fc(features)\n",
    "        return out, features\n",
    "\n",
    "\n",
    "def create_mobilenetv2(num_classes, pretrained=False):\n",
    "    \"\"\"\n",
    "    Creates MobileNetV2 model.\n",
    "    - ~3.4M parameters (vs VGG16's 138M)\n",
    "    - Uses inverted residuals and linear bottlenecks\n",
    "    - Ideal for small datasets\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "    \n",
    "    # Replace classifier for our number of classes\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return ModelWrapper(model, num_classes, in_features)\n",
    "\n",
    "\n",
    "def create_resnet18(num_classes, pretrained=False):\n",
    "    \"\"\"\n",
    "    Creates ResNet18 model.\n",
    "    - ~11M parameters\n",
    "    - Skip connections help with gradient flow\n",
    "    - Good for small-medium datasets\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        model = models.resnet18(weights=None)\n",
    "    \n",
    "    # Replace final fc layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return ModelWrapper(model, num_classes, in_features)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ARCHITECTURE REGISTRY\n",
    "# =============================================================================\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "ARCHITECTURES = {\n",
    "    # From scratch (random initialization)\n",
    "    'MobileNetV2_scratch': lambda: create_mobilenetv2(NUM_CLASSES, pretrained=False),\n",
    "    'ResNet18_scratch': lambda: create_resnet18(NUM_CLASSES, pretrained=False),\n",
    "    # Pretrained (ImageNet weights, fine-tuned)\n",
    "    'MobileNetV2_pretrained': lambda: create_mobilenetv2(NUM_CLASSES, pretrained=True),\n",
    "    'ResNet18_pretrained': lambda: create_resnet18(NUM_CLASSES, pretrained=True),\n",
    "}\n",
    "\n",
    "# Test model creation\n",
    "print(\"Model Architecture Summary:\")\n",
    "for name, create_fn in ARCHITECTURES.items():\n",
    "    model = create_fn()\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  {name}: {params:,} params ({trainable:,} trainable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8957e0b",
   "metadata": {},
   "source": [
    "## Data Augmentation Variants (A1-A4)\n",
    "\n",
    "Four augmentation strategies to compare:\n",
    "- **A1 (Baseline)**: Only resize + normalize (no augmentation)\n",
    "- **A2 (Geometric)**: Spatial transforms (crop, flip, rotation)\n",
    "- **A3 (Color)**: Color/intensity transforms (jitter, grayscale)\n",
    "- **A4 (Heavy)**: Combination of all augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUGMENTATION VARIANTS (A1-A4)\n",
    "# =============================================================================\n",
    "\n",
    "# ImageNet normalization (required for pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# A1: Baseline - No augmentation (only resize + normalize)\n",
    "aug_A1_baseline = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A2: Geometric - Spatial transformations\n",
    "aug_A2_geometric = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A3: Color - Color/intensity transformations  \n",
    "aug_A3_color = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A4: Heavy - All augmentations combined\n",
    "aug_A4_heavy = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation/Test transform (always deterministic - NO augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Dictionary for easy access\n",
    "AUGMENTATION_VARIANTS = {\n",
    "    'A1_baseline': aug_A1_baseline,\n",
    "    'A2_geometric': aug_A2_geometric,\n",
    "    'A3_color': aug_A3_color,\n",
    "    'A4_heavy': aug_A4_heavy,\n",
    "}\n",
    "\n",
    "print(\"Augmentation variants defined:\")\n",
    "for name in AUGMENTATION_VARIANTS:\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af81e3d",
   "metadata": {},
   "source": [
    "## Data Scenarios (E1-E4)\n",
    "\n",
    "Four training data scenarios as per requirements:\n",
    "- **E1 (Full)**: All available data (real + synthetic)\n",
    "- **E2 (Synthetic)**: Only synthetic/generated images\n",
    "- **E3 (Real)**: Only real photographs\n",
    "- **E4 (Balanced)**: Real data balanced to smallest class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9390b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA SCENARIOS (E1-E4)\n",
    "# =============================================================================\n",
    "\n",
    "DATA_SCENARIOS = {\n",
    "    'E1_full': 'dataset_split/full',           # Real + Synthetic combined\n",
    "    'E2_synthetic': 'dataset_split/synthetic', # Only synthetic\n",
    "    'E3_real': 'dataset_split/real',           # Only real\n",
    "    'E4_balanced': 'dataset_split/real',       # Balanced real (special handling)\n",
    "}\n",
    "\n",
    "# FIXED test set path - always use real test data for consistent evaluation\n",
    "FIXED_TEST_PATH = 'dataset_split/real/test'\n",
    "\n",
    "\n",
    "def get_balanced_indices(dataset, seed=SEED):\n",
    "    \"\"\"\n",
    "    Returns indices for a balanced subset where each class has equal samples.\n",
    "    Uses the minimum class count as the target.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    \n",
    "    # Group indices by class\n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Find minimum class size\n",
    "    min_count = min(len(indices) for indices in class_indices.values())\n",
    "    \n",
    "    # Sample equally from each class\n",
    "    balanced_indices = []\n",
    "    for indices in class_indices.values():\n",
    "        sampled = rng.sample(indices, min_count) if len(indices) > min_count else indices\n",
    "        balanced_indices.extend(sampled)\n",
    "    \n",
    "    return balanced_indices, min_count\n",
    "\n",
    "\n",
    "def create_data_loaders(scenario, batch_size, aug_variant='A1_baseline'):\n",
    "    \"\"\"\n",
    "    Creates train, val, test loaders for a given scenario and augmentation.\n",
    "    \n",
    "    CRITICAL: Test loader ALWAYS uses the same fixed test set (real images only)\n",
    "    with deterministic transforms for consistent evaluation across experiments.\n",
    "    \"\"\"\n",
    "    train_transform = AUGMENTATION_VARIANTS.get(aug_variant, aug_A1_baseline)\n",
    "    \n",
    "    if scenario == 'E4_balanced':\n",
    "        # Special handling for balanced dataset\n",
    "        data_dir = DATA_SCENARIOS['E3_real']\n",
    "        train_path = os.path.join(data_dir, 'train')\n",
    "        \n",
    "        full_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
    "        balanced_indices, samples_per_class = get_balanced_indices(full_dataset)\n",
    "        \n",
    "        train_dataset = Subset(full_dataset, balanced_indices)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True, \n",
    "                                  num_workers=2, pin_memory=True)\n",
    "        train_size = len(train_dataset)\n",
    "        class_names = full_dataset.classes\n",
    "        \n",
    "    else:\n",
    "        data_dir = DATA_SCENARIOS[scenario]\n",
    "        train_path = os.path.join(data_dir, 'train')\n",
    "        \n",
    "        train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True,\n",
    "                                  num_workers=2, pin_memory=True)\n",
    "        train_size = len(train_dataset)\n",
    "        class_names = train_dataset.classes\n",
    "    \n",
    "    # Validation set - use real data with deterministic transforms\n",
    "    val_path = os.path.join(DATA_SCENARIOS['E3_real'], 'val')\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=val_test_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False,\n",
    "                           num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # TEST SET - CRITICAL: Always the same fixed test set!\n",
    "    test_dataset = datasets.ImageFolder(FIXED_TEST_PATH, transform=val_test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False,\n",
    "                            num_workers=2, pin_memory=True)\n",
    "    \n",
    "    sizes = {\n",
    "        'train': train_size,\n",
    "        'val': len(val_dataset),\n",
    "        'test': len(test_dataset)\n",
    "    }\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_names, sizes\n",
    "\n",
    "\n",
    "# Verify data scenarios and fixed test set\n",
    "print(\"Data Scenarios Summary:\")\n",
    "print(f\"   Fixed Test Set: {FIXED_TEST_PATH}\")\n",
    "\n",
    "for scenario, path in DATA_SCENARIOS.items():\n",
    "    train_path = os.path.join(path, 'train')\n",
    "    if os.path.exists(train_path):\n",
    "        dataset = datasets.ImageFolder(train_path, transform=val_test_transform)\n",
    "        print(f\"   {scenario}: {len(dataset)} train images, classes={dataset.classes}\")\n",
    "\n",
    "# Verify test set\n",
    "test_dataset = datasets.ImageFolder(FIXED_TEST_PATH, transform=val_test_transform)\n",
    "print(f\"\\nFIXED Test Set: {len(test_dataset)} images (must be 20)\")\n",
    "for class_name in test_dataset.classes:\n",
    "    class_count = len([s for s in test_dataset.samples if test_dataset.classes[s[1]] == class_name])\n",
    "    # This counts correctly\n",
    "test_counts = {c: 0 for c in test_dataset.classes}\n",
    "for _, label in test_dataset.samples:\n",
    "    test_counts[test_dataset.classes[label]] += 1\n",
    "print(f\"   Per class: {test_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f136a1",
   "metadata": {},
   "source": [
    "## Training & Evaluation Functions\n",
    "\n",
    "Training configuration optimized for small datasets:\n",
    "- **Learning rate**: 1e-3 (scratch) or 1e-4 (fine-tuning)  \n",
    "- **Weight decay**: 1e-4 for regularization\n",
    "- **Early stopping**: Based on validation accuracy\n",
    "- **Epochs**: 30 (enough for small dataset convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'num_classes': 5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 16,  # Small batch for small dataset\n",
    "    'learning_rate_scratch': 1e-3,    # Higher LR for training from scratch\n",
    "    'learning_rate_finetune': 1e-4,   # Lower LR for fine-tuning pretrained\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 7,  # Early stopping patience\n",
    "}\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "for k, v in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device=DEVICE, patience=7, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains a model with early stopping based on validation accuracy.\n",
    "    Returns the best model state and training history.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ===== Training Phase =====\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / max(total, 1)\n",
    "        train_acc = 100.0 * correct / max(total, 1)\n",
    "        \n",
    "        # ===== Validation Phase =====\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs, _ = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / max(val_total, 1)\n",
    "        val_acc = 100.0 * val_correct / max(val_total, 1)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "                  f\"Train: {train_loss:.4f}/{train_acc:.1f}% | \"\n",
    "                  f\"Val: {val_loss:.4f}/{val_acc:.1f}% | \"\n",
    "                  f\"Best: {best_val_acc:.1f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluates model on the FIXED test set.\n",
    "    Returns accuracy, confusion matrix, precision, recall, F1.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = 100.0 * (all_preds == all_labels).sum() / len(all_labels)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1_score': f1 * 100,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', ax=None):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"Plots training and validation loss/accuracy curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{title} - Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title(f'{title} - Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"Training & Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f428a2",
   "metadata": {},
   "source": [
    "## Experiment Runner\n",
    "\n",
    "The experiment runner executes all 32 experiments systematically:\n",
    "- 2 architectures √ó 4 data scenarios √ó 4 augmentation variants = 32 experiments\n",
    "- Plus comparison with pretrained models adds another dimension\n",
    "\n",
    "Each experiment:\n",
    "1. Creates fresh model instance\n",
    "2. Loads appropriate data loaders\n",
    "3. Trains with early stopping\n",
    "4. Evaluates on FIXED test set\n",
    "5. Records all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40950cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT RUNNER\n",
    "# =============================================================================\n",
    "\n",
    "def run_single_experiment(arch_name, scenario, aug_variant, config=EXPERIMENT_CONFIG, verbose=True):\n",
    "    \"\"\"\n",
    "    Runs a single experiment and returns results.\n",
    "    \"\"\"\n",
    "    set_seed(SEED)  # Reset seed for reproducibility\n",
    "    \n",
    "    # Determine if this is a pretrained model\n",
    "    is_pretrained = 'pretrained' in arch_name\n",
    "    learning_rate = config['learning_rate_finetune'] if is_pretrained else config['learning_rate_scratch']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"   Experiment: {arch_name} | {scenario} | {aug_variant}\")\n",
    "    print(f\"   Device: {DEVICE} | LR: {learning_rate} | Pretrained: {is_pretrained}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader, class_names, sizes = create_data_loaders(\n",
    "        scenario, config['batch_size'], aug_variant\n",
    "    )\n",
    "    print(f\"   Data: train={sizes['train']}, val={sizes['val']}, test={sizes['test']}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = ARCHITECTURES[arch_name]()\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model, history, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        config['num_epochs'], DEVICE, config['patience'], verbose=verbose\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on FIXED test set\n",
    "    metrics = evaluate_model(model, test_loader, class_names, DEVICE)\n",
    "    \n",
    "    print(f\"\\nTest Results (on FIXED test set of {sizes['test']} images):\")\n",
    "    print(f\"   Accuracy:  {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"   Precision: {metrics['precision']:.2f}%\")\n",
    "    print(f\"   Recall:    {metrics['recall']:.2f}%\")\n",
    "    print(f\"   F1 Score:  {metrics['f1_score']:.2f}%\")\n",
    "    print(f\"   Time:      {train_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'architecture': arch_name,\n",
    "        'scenario': scenario,\n",
    "        'augmentation': aug_variant,\n",
    "        'pretrained': is_pretrained,\n",
    "        'train_size': sizes['train'],\n",
    "        'val_size': sizes['val'],\n",
    "        'test_size': sizes['test'],\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_accuracy': metrics['accuracy'],\n",
    "        'test_precision': metrics['precision'],\n",
    "        'test_recall': metrics['recall'],\n",
    "        'test_f1': metrics['f1_score'],\n",
    "        'train_time': train_time,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'confusion_matrix': metrics['confusion_matrix'],\n",
    "        'class_names': class_names,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_all_experiments(architectures=None, scenarios=None, augmentations=None, \n",
    "                       config=EXPERIMENT_CONFIG, save_models=True):\n",
    "    \"\"\"\n",
    "    Runs all experiments and returns results DataFrame.\n",
    "    \n",
    "    Default: 4 architectures √ó 4 scenarios √ó 4 augmentations = 64 experiments\n",
    "    For just scratch comparison: 2 architectures √ó 4 scenarios √ó 4 augmentations = 32 experiments\n",
    "    \"\"\"\n",
    "    if architectures is None:\n",
    "        architectures = list(ARCHITECTURES.keys())\n",
    "    if scenarios is None:\n",
    "        scenarios = list(DATA_SCENARIOS.keys())\n",
    "    if augmentations is None:\n",
    "        augmentations = list(AUGMENTATION_VARIANTS.keys())\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    total = len(architectures) * len(scenarios) * len(augmentations)\n",
    "    print(f\"\\n Running {total} experiments...\")\n",
    "    print(f\"   Architectures: {architectures}\")\n",
    "    print(f\"   Scenarios: {scenarios}\")\n",
    "    print(f\"   Augmentations: {augmentations}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        for scenario in scenarios:\n",
    "            for aug in augmentations:\n",
    "                try:\n",
    "                    result = run_single_experiment(arch, scenario, aug, config, verbose=False)\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    if save_models:\n",
    "                        model_name = f\"{arch}_{scenario}_{aug}.pth\"\n",
    "                        torch.save(result['model'].state_dict(), f\"models/{model_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\" ERROR in {arch}/{scenario}/{aug}: {e}\")\n",
    "                    all_results.append({\n",
    "                        'architecture': arch,\n",
    "                        'scenario': scenario,\n",
    "                        'augmentation': aug,\n",
    "                        'error': str(e),\n",
    "                    })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for r in all_results:\n",
    "        if 'error' not in r:\n",
    "            summary_data.append({\n",
    "                'Architecture': r['architecture'],\n",
    "                'Pretrained': r['pretrained'],\n",
    "                'Scenario': r['scenario'],\n",
    "                'Augmentation': r['augmentation'],\n",
    "                'Train Size': r['train_size'],\n",
    "                'Best Val Acc (%)': round(r['best_val_acc'], 2),\n",
    "                'Test Acc (%)': round(r['test_accuracy'], 2),\n",
    "                'Test Precision (%)': round(r['test_precision'], 2),\n",
    "                'Test Recall (%)': round(r['test_recall'], 2),\n",
    "                'Test F1 (%)': round(r['test_f1'], 2),\n",
    "                'Train Time (s)': round(r['train_time'], 1),\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(summary_data)\n",
    "    return results_df, all_results\n",
    "\n",
    "print(\" Experiment runner configured\")\n",
    "print(f\"   Total architectures: {len(ARCHITECTURES)}\")\n",
    "print(f\"   Total scenarios: {len(DATA_SCENARIOS)}\")\n",
    "print(f\"   Total augmentations: {len(AUGMENTATION_VARIANTS)}\")\n",
    "print(f\"   Max experiments: {len(ARCHITECTURES) * len(DATA_SCENARIOS) * len(AUGMENTATION_VARIANTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d0f53",
   "metadata": {},
   "source": [
    "## Quick Test: Single Experiment\n",
    "\n",
    "Before running all experiments, verify everything works with a single test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a150a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with a single experiment (MobileNetV2 scratch, real data, baseline augmentation)\n",
    "test_config = {**EXPERIMENT_CONFIG, 'num_epochs': 5}  # Short run for testing\n",
    "\n",
    "test_result = run_single_experiment(\n",
    "    'MobileNetV2_scratch', 'E3_real', 'A1_baseline', \n",
    "    config=test_config, verbose=True\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    test_result['confusion_matrix'], \n",
    "    test_result['class_names'],\n",
    "    title='MobileNetV2 (scratch) - Quick Test',\n",
    "    ax=axes[0]\n",
    ")\n",
    "\n",
    "# Training history\n",
    "axes[1].plot(test_result['history']['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(test_result['history']['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training History')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nQuick test complete! Test accuracy: {test_result['test_accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff6c29",
   "metadata": {},
   "source": [
    "## Run All 32 Experiments (Scratch Models Only)\n",
    "\n",
    "This runs the required 32 experiments:\n",
    "- 2 architectures (MobileNetV2, ResNet18) - both from scratch\n",
    "- 4 data scenarios (E1-E4)\n",
    "- 4 augmentation variants (A1-A4)\n",
    "\n",
    "‚è±Ô∏è Estimated time: ~15-30 minutes on GPU, ~60-120 minutes on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ALL 32 EXPERIMENTS (Scratch Models Only - as per requirements)\n",
    "# =============================================================================\n",
    "\n",
    "# Run experiments with scratch models only (2 arch √ó 4 scenarios √ó 4 aug = 32)\n",
    "scratch_architectures = ['MobileNetV2_scratch', 'ResNet18_scratch']\n",
    "\n",
    "results_scratch_df, results_scratch = run_all_experiments(\n",
    "    architectures=scratch_architectures,\n",
    "    scenarios=list(DATA_SCENARIOS.keys()),\n",
    "    augmentations=list(AUGMENTATION_VARIANTS.keys()),\n",
    "    config=EXPERIMENT_CONFIG,\n",
    "    save_models=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_scratch_df.to_csv('experiment_results_scratch.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCRATCH MODELS - RESULTS SUMMARY (32 Experiments)\")\n",
    "print(\"=\"*80)\n",
    "print(results_scratch_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce965e",
   "metadata": {},
   "source": [
    "## Run Pretrained/Fine-tuned Comparison\n",
    "\n",
    "Now run the same experiments with pretrained (ImageNet) models to compare transfer learning vs training from scratch.\n",
    "\n",
    "This is crucial for small datasets - pretrained features often dramatically improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN PRETRAINED EXPERIMENTS (Transfer Learning Comparison)\n",
    "# =============================================================================\n",
    "\n",
    "# Run experiments with pretrained models (2 arch √ó 4 scenarios √ó 4 aug = 32)\n",
    "pretrained_architectures = ['MobileNetV2_pretrained', 'ResNet18_pretrained']\n",
    "\n",
    "results_pretrained_df, results_pretrained = run_all_experiments(\n",
    "    architectures=pretrained_architectures,\n",
    "    scenarios=list(DATA_SCENARIOS.keys()),\n",
    "    augmentations=list(AUGMENTATION_VARIANTS.keys()),\n",
    "    config=EXPERIMENT_CONFIG,\n",
    "    save_models=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_pretrained_df.to_csv('experiment_results_pretrained.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRETRAINED MODELS - RESULTS SUMMARY (32 Experiments)\")\n",
    "print(\"=\"*80)\n",
    "print(results_pretrained_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077d6be",
   "metadata": {},
   "source": [
    "## Results Visualization & Analysis\n",
    "\n",
    "Compare performance across:\n",
    "1. Scratch vs Pretrained models\n",
    "2. Different architectures\n",
    "3. Different data scenarios\n",
    "4. Different augmentation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10857c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE RESULTS VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Combine all results\n",
    "all_results_df = pd.concat([results_scratch_df, results_pretrained_df], ignore_index=True)\n",
    "all_results_df.to_csv('experiment_results_all.csv', index=False)\n",
    "all_results_list = results_scratch + results_pretrained\n",
    "\n",
    "print(f\"Total experiments completed: {len(all_results_df)}\")\n",
    "\n",
    "\n",
    "def plot_comprehensive_comparison(df):\n",
    "    \"\"\"Creates comprehensive comparison charts.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Scratch vs Pretrained comparison\n",
    "    pretrained_comparison = df.groupby('Pretrained')['Test Acc (%)'].mean()\n",
    "    colors = ['#ff6b6b', '#4ecdc4']\n",
    "    bars = axes[0, 0].bar(['Scratch', 'Pretrained'], pretrained_comparison.values, color=colors)\n",
    "    axes[0, 0].set_title('Scratch vs Pretrained\\n(Average Test Accuracy)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].set_ylim(0, 100)\n",
    "    for bar, val in zip(bars, pretrained_comparison.values):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                       f'{val:.1f}%', ha='center', fontsize=11)\n",
    "    \n",
    "    # 2. Architecture comparison (grouped by pretrained)\n",
    "    arch_data = df.groupby(['Architecture', 'Pretrained'])['Test Acc (%)'].mean().unstack()\n",
    "    arch_data.plot(kind='bar', ax=axes[0, 1], color=colors, width=0.7)\n",
    "    axes[0, 1].set_title('Architecture Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].set_xticklabels([a.replace('_scratch', '').replace('_pretrained', '') \n",
    "                                for a in arch_data.index], rotation=0)\n",
    "    axes[0, 1].legend(['Scratch', 'Pretrained'], loc='lower right')\n",
    "    axes[0, 1].set_ylim(0, 100)\n",
    "    \n",
    "    # 3. Data Scenario comparison\n",
    "    scenario_data = df.groupby(['Scenario', 'Pretrained'])['Test Acc (%)'].mean().unstack()\n",
    "    scenario_data.plot(kind='bar', ax=axes[0, 2], color=colors, width=0.7)\n",
    "    axes[0, 2].set_title('Data Scenario Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 2].set_xticklabels(scenario_data.index, rotation=45, ha='right')\n",
    "    axes[0, 2].legend(['Scratch', 'Pretrained'], loc='lower right')\n",
    "    axes[0, 2].set_ylim(0, 100)\n",
    "    \n",
    "    # 4. Augmentation comparison\n",
    "    aug_data = df.groupby(['Augmentation', 'Pretrained'])['Test Acc (%)'].mean().unstack()\n",
    "    aug_data.plot(kind='bar', ax=axes[1, 0], color=colors, width=0.7)\n",
    "    axes[1, 0].set_title('Augmentation Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].set_xticklabels(aug_data.index, rotation=45, ha='right')\n",
    "    axes[1, 0].legend(['Scratch', 'Pretrained'], loc='lower right')\n",
    "    axes[1, 0].set_ylim(0, 100)\n",
    "    \n",
    "    # 5. Heatmap: Architecture x Scenario (Scratch only)\n",
    "    scratch_df = df[df['Pretrained'] == False]\n",
    "    if len(scratch_df) > 0:\n",
    "        # Extract base architecture name\n",
    "        scratch_df = scratch_df.copy()\n",
    "        scratch_df['Arch_Base'] = scratch_df['Architecture'].str.replace('_scratch', '')\n",
    "        pivot = scratch_df.pivot_table(\n",
    "            values='Test Acc (%)', \n",
    "            index='Arch_Base', \n",
    "            columns='Scenario', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        im = axes[1, 1].imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "        axes[1, 1].set_xticks(range(len(pivot.columns)))\n",
    "        axes[1, 1].set_xticklabels(pivot.columns, rotation=45, ha='right')\n",
    "        axes[1, 1].set_yticks(range(len(pivot.index)))\n",
    "        axes[1, 1].set_yticklabels(pivot.index)\n",
    "        axes[1, 1].set_title('Scratch Models: Arch √ó Scenario', fontsize=12, fontweight='bold')\n",
    "        plt.colorbar(im, ax=axes[1, 1], label='Accuracy (%)')\n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                axes[1, 1].text(j, i, f'{pivot.values[i, j]:.1f}%', \n",
    "                               ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 6. Heatmap: Architecture x Scenario (Pretrained)\n",
    "    pretrained_df = df[df['Pretrained'] == True]\n",
    "    if len(pretrained_df) > 0:\n",
    "        pretrained_df = pretrained_df.copy()\n",
    "        pretrained_df['Arch_Base'] = pretrained_df['Architecture'].str.replace('_pretrained', '')\n",
    "        pivot = pretrained_df.pivot_table(\n",
    "            values='Test Acc (%)', \n",
    "            index='Arch_Base', \n",
    "            columns='Scenario', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        im = axes[1, 2].imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "        axes[1, 2].set_xticks(range(len(pivot.columns)))\n",
    "        axes[1, 2].set_xticklabels(pivot.columns, rotation=45, ha='right')\n",
    "        axes[1, 2].set_yticks(range(len(pivot.index)))\n",
    "        axes[1, 2].set_yticklabels(pivot.index)\n",
    "        axes[1, 2].set_title('Pretrained Models: Arch √ó Scenario', fontsize=12, fontweight='bold')\n",
    "        plt.colorbar(im, ax=axes[1, 2], label='Accuracy (%)')\n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                axes[1, 2].text(j, i, f'{pivot.values[i, j]:.1f}%', \n",
    "                               ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_all_confusion_matrices(all_results, title_prefix=''):\n",
    "    \"\"\"Plots confusion matrices for all experiments.\"\"\"\n",
    "    valid_results = [r for r in all_results if 'error' not in r]\n",
    "    n = len(valid_results)\n",
    "    if n == 0:\n",
    "        print(\"No valid results to plot.\")\n",
    "        return\n",
    "    \n",
    "    n_cols = 4\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, result in enumerate(valid_results):\n",
    "        row, col = idx // n_cols, idx % n_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        cm = result['confusion_matrix']\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=result['class_names'])\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=False)\n",
    "        \n",
    "        pretrained_str = '(PT)' if result.get('pretrained', False) else '(S)'\n",
    "        arch_short = result['architecture'].replace('_scratch', '').replace('_pretrained', '')\n",
    "        ax.set_title(f\"{arch_short} {pretrained_str}\\n{result['scenario']}\\n{result['augmentation']}\", \n",
    "                    fontsize=8)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n, n_rows * n_cols):\n",
    "        row, col = idx // n_cols, idx % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} Confusion Matrices', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate visualizations\n",
    "if len(all_results_df) > 0:\n",
    "    print(\"\\nGenerating comparison charts...\")\n",
    "    plot_comprehensive_comparison(all_results_df)\n",
    "    plt.savefig('comparison_charts.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConfusion matrices for scratch models...\")\n",
    "    plot_all_confusion_matrices(results_scratch, 'Scratch')\n",
    "    plt.savefig('confusion_matrices_scratch.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConfusion matrices for pretrained models...\")\n",
    "    plot_all_confusion_matrices(results_pretrained, 'Pretrained')\n",
    "    plt.savefig('confusion_matrices_pretrained.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150fda3",
   "metadata": {},
   "source": [
    "## Final Summary & Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY & ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"   Total experiments: {len(all_results_df)}\")\n",
    "print(f\"   Test set size: 20 images (FIXED across all experiments)\")\n",
    "\n",
    "# Best results by category\n",
    "print(f\"\\nBEST RESULTS:\")\n",
    "\n",
    "# Best scratch model\n",
    "scratch_df = all_results_df[all_results_df['Pretrained'] == False]\n",
    "if len(scratch_df) > 0:\n",
    "    best_scratch = scratch_df.loc[scratch_df['Test Acc (%)'].idxmax()]\n",
    "    print(f\"\\n   Best SCRATCH Model:\")\n",
    "    print(f\"      Architecture: {best_scratch['Architecture']}\")\n",
    "    print(f\"      Scenario: {best_scratch['Scenario']}\")\n",
    "    print(f\"      Augmentation: {best_scratch['Augmentation']}\")\n",
    "    print(f\"      Test Accuracy: {best_scratch['Test Acc (%)']:.2f}%\")\n",
    "    print(f\"      F1 Score: {best_scratch['Test F1 (%)']:.2f}%\")\n",
    "\n",
    "# Best pretrained model\n",
    "pretrained_df = all_results_df[all_results_df['Pretrained'] == True]\n",
    "if len(pretrained_df) > 0:\n",
    "    best_pretrained = pretrained_df.loc[pretrained_df['Test Acc (%)'].idxmax()]\n",
    "    print(f\"\\n   Best PRETRAINED Model:\")\n",
    "    print(f\"      Architecture: {best_pretrained['Architecture']}\")\n",
    "    print(f\"      Scenario: {best_pretrained['Scenario']}\")\n",
    "    print(f\"      Augmentation: {best_pretrained['Augmentation']}\")\n",
    "    print(f\"      Test Accuracy: {best_pretrained['Test Acc (%)']:.2f}%\")\n",
    "    print(f\"      F1 Score: {best_pretrained['Test F1 (%)']:.2f}%\")\n",
    "\n",
    "# Improvement from pretraining\n",
    "if len(scratch_df) > 0 and len(pretrained_df) > 0:\n",
    "    avg_scratch = scratch_df['Test Acc (%)'].mean()\n",
    "    avg_pretrained = pretrained_df['Test Acc (%)'].mean()\n",
    "    improvement = avg_pretrained - avg_scratch\n",
    "    print(f\"\\nTransfer Learning Improvement:\")\n",
    "    print(f\"   Average Scratch Accuracy: {avg_scratch:.2f}%\")\n",
    "    print(f\"   Average Pretrained Accuracy: {avg_pretrained:.2f}%\")\n",
    "    print(f\"   Improvement: +{improvement:.2f}%\")\n",
    "\n",
    "# Summary by scenario\n",
    "print(f\"\\nAverage Accuracy by Data Scenario:\")\n",
    "scenario_summary = all_results_df.groupby(['Scenario', 'Pretrained'])['Test Acc (%)'].mean().unstack()\n",
    "print(scenario_summary.to_string())\n",
    "\n",
    "# Summary by augmentation\n",
    "print(f\"\\nAverage Accuracy by Augmentation:\")\n",
    "aug_summary = all_results_df.groupby(['Augmentation', 'Pretrained'])['Test Acc (%)'].mean().unstack()\n",
    "print(aug_summary.to_string())\n",
    "\n",
    "# Save comprehensive results\n",
    "all_results_df.to_csv('experiment_results_final.csv', index=False)\n",
    "print(f\"\\nResults saved to experiment_results_final.csv\")\n",
    "\n",
    "# Save best model\n",
    "valid_results = [r for r in all_results_list if 'error' not in r and 'test_accuracy' in r]\n",
    "if valid_results:\n",
    "    best = max(valid_results, key=lambda x: x['test_accuracy'])\n",
    "    best_name = f\"BEST_{best['architecture']}_{best['scenario']}_{best['augmentation']}.pth\"\n",
    "    torch.save(best['model'].state_dict(), f\"models/{best_name}\")\n",
    "    print(f\"üèÜ Best model saved: models/{best_name}\")\n",
    "    print(f\"   Test Accuracy: {best['test_accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860689b",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Pretrained vs Scratch**: For small datasets (~50-100 training images), pretrained models with fine-tuning significantly outperform models trained from scratch. Transfer learning leverages features learned from millions of ImageNet images.\n",
    "\n",
    "2. **Architecture Choice**: MobileNetV2 and ResNet18 are appropriate for this dataset size. VGG16 (138M params) would severely overfit with so few samples.\n",
    "\n",
    "3. **Data Scenarios**: \n",
    "   - E3 (Real only) typically performs best since test set is real images\n",
    "   - E1 (Full) can help when augmented well\n",
    "   - E2 (Synthetic) shows the domain gap between synthetic and real images\n",
    "   - E4 (Balanced) ensures no class imbalance issues\n",
    "\n",
    "4. **Augmentation Impact**: Heavy augmentation (A4) often helps scratch models more than pretrained models, which already have robust features.\n",
    "\n",
    "### Recommendations:\n",
    "- For production: Use **pretrained MobileNetV2/ResNet18** with heavy augmentation\n",
    "- For limited compute: MobileNetV2 offers best accuracy/parameter ratio\n",
    "- Always validate on real images even when training includes synthetic data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
