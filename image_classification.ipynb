{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a11fc11",
   "metadata": {},
   "source": [
    "# VGG classifier training and fine tunning comparison\n",
    "\n",
    "In this notebook I want to compare the accuracy of a VGG model build from scratch and trained on the target dataset (bike category recognition) with a pre trained model (ImageNet) and fine tunned model.\n",
    "\n",
    "I'll be using discriminative fine-tunning in this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6835682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(94417) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: numpy==1.24.0 in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.24.0)\n",
      "Requirement already satisfied: torch in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.17.2)\n",
      "Requirement already satisfied: pydantic in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.12.5)\n",
      "Requirement already satisfied: tqdm in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.7.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv1/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.10.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv1/lib/python3.11/site-packages (from requests->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv1/lib/python3.11/site-packages (from requests->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv1/lib/python3.11/site-packages (from requests->-r requirements.txt (line 1)) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv1/lib/python3.11/site-packages (from requests->-r requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: filelock in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: sympy in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv1/lib/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (2026.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv1/lib/python3.11/site-packages (from torchvision->-r requirements.txt (line 5)) (12.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv1/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv1/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv1/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (0.4.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv1/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (26.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv1/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv1/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv1/lib/python3.11/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv1/lib/python3.11/site-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ce4c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  Detected classes: ['cargo', 'road', 'fold', 'mtb', 'hybrid']\n",
      "ðŸ”„ Processing: cargo...\n",
      "ðŸ”„ Processing: road...\n",
      "ðŸ”„ Processing: fold...\n",
      "ðŸ”„ Processing: mtb...\n",
      "ðŸ”„ Processing: hybrid...\n",
      "\n",
      "âœ… Done! Dataset organized at: dataset_split\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils.load_dataset import load_dataset\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "dataset_url = os.getenv(\"DATASET_REPO_URL\")\n",
    "\n",
    "load_dataset(dataset_url, extract_dir=\"dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a9d1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8178dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd9a0",
   "metadata": {},
   "source": [
    "## Architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2765edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8df98",
   "metadata": {},
   "source": [
    "### VGG configuration arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b92676af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512,\n",
    "                512, 'M']\n",
    "\n",
    "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512,\n",
    "                'M', 512, 512, 512, 'M']\n",
    "\n",
    "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512,\n",
    "                512, 512, 'M', 512, 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e453f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_layers(config, batch_norm):\n",
    "\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        if c == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = c\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8957e0b",
   "metadata": {},
   "source": [
    "### ResNet-18 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "529fcefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 created with 11,179,077 parameters\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def get_resnet18(num_classes, pretrained=False):\n",
    "    \"\"\"\n",
    "    Returns a ResNet-18 model with modified final layer for num_classes.\n",
    "    Skip connections differentiate it from VGG's plain architecture.\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        model = models.resnet18(weights=None)\n",
    "    \n",
    "    # Replace final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Wrapper to match VGG interface (returns outputs, hidden)\n",
    "class ResNetWrapper(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.resnet = get_resnet18(num_classes, pretrained)\n",
    "        # Store feature extractor (everything except fc)\n",
    "        self.features = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        self.fc = self.resnet.fc\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        h = self.features(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        # Classify\n",
    "        out = self.fc(h)\n",
    "        return out, h\n",
    "\n",
    "# Test instantiation\n",
    "resnet18_model = ResNetWrapper(num_classes=5, pretrained=False)\n",
    "print(f\"ResNet-18 created with {sum(p.numel() for p in resnet18_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80407639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU(inplace=True)\n",
      "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n",
    "\n",
    "vgg16_layers = get_vgg_layers(vgg16_config, batch_norm=True)\n",
    "\n",
    "print(vgg11_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8456b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation variants defined:\n",
      "  - A1_baseline\n",
      "  - A2_geometric\n",
      "  - A3_color\n",
      "  - A4_heavy\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# =============================================================================\n",
    "# 4 AUGMENTATION VARIANTS (A1-A4)\n",
    "# =============================================================================\n",
    "\n",
    "# A1: Baseline - No augmentation (only resize + normalize)\n",
    "aug_A1_baseline = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A2: Geometric - Spatial transformations\n",
    "aug_A2_geometric = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A3: Color - Color/intensity transformations\n",
    "aug_A3_color = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# A4: Heavy - All augmentations combined\n",
    "aug_A4_heavy = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation/Test transform (always deterministic)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Dictionary for easy access\n",
    "AUGMENTATION_VARIANTS = {\n",
    "    'A1_baseline': aug_A1_baseline,\n",
    "    'A2_geometric': aug_A2_geometric,\n",
    "    'A3_color': aug_A3_color,\n",
    "    'A4_heavy': aug_A4_heavy,\n",
    "}\n",
    "\n",
    "print(\"Augmentation variants defined:\")\n",
    "for name in AUGMENTATION_VARIANTS:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af81e3d",
   "metadata": {},
   "source": [
    "## Data Scenarios (E1-E4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de9390b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scenarios:\n",
      "  E1_full: 97 train samples, classes=['cargo', 'fold', 'hybrid', 'mtb', 'road']\n",
      "  E2_synthetic: 50 train samples, classes=['cargo', 'fold', 'hybrid', 'mtb', 'road']\n",
      "  E3_real: 48 train samples, classes=['cargo', 'fold', 'hybrid', 'mtb', 'road']\n",
      "  E4_balanced: 48 train samples, classes=['cargo', 'fold', 'hybrid', 'mtb', 'road']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA SCENARIOS (E1-E4)\n",
    "# =============================================================================\n",
    "\n",
    "DATA_SCENARIOS = {\n",
    "    'E1_full': 'dataset_split/full',        # All data (real + synthetic)\n",
    "    'E2_synthetic': 'dataset_split/synthetic',  # Only synthetic\n",
    "    'E3_real': 'dataset_split/real',        # Only real\n",
    "    'E4_balanced': 'dataset_split/real',    # Balanced (handled separately)\n",
    "}\n",
    "\n",
    "def data_loader(data_dir, batch_size, set_type='train', shuffle=True, transform=None):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the specified dataset split.\n",
    "    \"\"\"\n",
    "    if transform is None:\n",
    "        transform = val_transforms\n",
    "    set_path = os.path.join(data_dir, set_type)\n",
    "    dataset = datasets.ImageFolder(set_path, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=shuffle)\n",
    "    size = len(dataset)\n",
    "    return loader, size, dataset.classes\n",
    "\n",
    "def get_balanced_subset(data_dir, set_type='train', target_size_per_class=None):\n",
    "    \"\"\"\n",
    "    Creates a balanced subset by limiting each class to target_size_per_class.\n",
    "    If target_size_per_class is None, uses the minimum class size.\n",
    "    \"\"\"\n",
    "    set_path = os.path.join(data_dir, set_type)\n",
    "    dataset = datasets.ImageFolder(set_path, transform=None)\n",
    "    \n",
    "    # Group samples by class\n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Find minimum class size if not specified\n",
    "    if target_size_per_class is None:\n",
    "        target_size_per_class = min(len(indices) for indices in class_indices.values())\n",
    "    \n",
    "    # Sample equally from each class\n",
    "    balanced_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        if len(indices) >= target_size_per_class:\n",
    "            balanced_indices.extend(random.sample(indices, target_size_per_class))\n",
    "        else:\n",
    "            balanced_indices.extend(indices)  # Use all if fewer\n",
    "    \n",
    "    return balanced_indices, target_size_per_class\n",
    "\n",
    "def create_data_loaders(scenario, batch_size, aug_variant='A1_baseline'):\n",
    "    \"\"\"\n",
    "    Creates train, val, test loaders for a given scenario and augmentation.\n",
    "    \n",
    "    Returns: train_loader, val_loader, test_loader, class_names, sizes_dict\n",
    "    \"\"\"\n",
    "    train_transform = AUGMENTATION_VARIANTS.get(aug_variant, aug_A1_baseline)\n",
    "    \n",
    "    if scenario == 'E4_balanced':\n",
    "        # Special handling for balanced dataset\n",
    "        data_dir = DATA_SCENARIOS['E3_real']  # Base on real data\n",
    "        \n",
    "        # Get balanced indices\n",
    "        balanced_indices, samples_per_class = get_balanced_subset(data_dir, 'train')\n",
    "        \n",
    "        # Create dataset with transform\n",
    "        train_path = os.path.join(data_dir, 'train')\n",
    "        full_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, balanced_indices)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        train_size = len(train_dataset)\n",
    "        class_names = full_dataset.classes\n",
    "        \n",
    "    else:\n",
    "        data_dir = DATA_SCENARIOS[scenario]\n",
    "        train_loader, train_size, class_names = data_loader(\n",
    "            data_dir, batch_size, 'train', shuffle=True, transform=train_transform\n",
    "        )\n",
    "    \n",
    "    # Val and test always use real data with deterministic transforms\n",
    "    val_loader, val_size, _ = data_loader(\n",
    "        DATA_SCENARIOS['E3_real'], batch_size, 'val', shuffle=False, transform=val_transforms\n",
    "    )\n",
    "    test_loader, test_size, _ = data_loader(\n",
    "        DATA_SCENARIOS['E3_real'], batch_size, 'test', shuffle=False, transform=val_transforms\n",
    "    )\n",
    "    \n",
    "    sizes = {'train': train_size, 'val': val_size, 'test': test_size}\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_names, sizes\n",
    "\n",
    "# Test data scenarios\n",
    "print(\"Data Scenarios:\")\n",
    "for scenario, path in DATA_SCENARIOS.items():\n",
    "    if os.path.exists(os.path.join(path, 'train')):\n",
    "        _, size, classes = data_loader(path, 1, 'train')\n",
    "        print(f\"  {scenario}: {size} train samples, classes={classes}\")\n",
    "    else:\n",
    "        print(f\"  {scenario}: path not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f136a1",
   "metadata": {},
   "source": [
    "## Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24ab7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = VGG(vgg11_layers, num_classes)\n",
    "\n",
    "model16 = VGG(vgg16_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "optimizer16 = torch.optim.Adam(model16.parameters(), lr=learning_rate, weight_decay = weight_decay)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9ec43",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01a1d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, device=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains a model and returns training history.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / max(total, 1)\n",
    "        train_acc = 100.0 * correct / max(total, 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs, _ = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / max(val_total, 1)\n",
    "        val_acc = 100.0 * val_correct / max(val_total, 1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"train_loss={train_loss:.4f} train_acc={train_acc:.2f}% | \"\n",
    "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c303a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on test set and returns metrics + confusion matrix.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = 100.0 * (all_preds == all_labels).sum() / len(all_labels)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1_score': f1 * 100,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss/accuracy curves.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{title} - Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title(f'{title} - Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"Training & Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f428a2",
   "metadata": {},
   "source": [
    "## Experiment Runner (32 Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff6a5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(96336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-3.0.0-cp311-cp311-macosx_10_9_x86_64.whl (10.3 MB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp311-cp311-macosx_14_0_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv1/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.0\n",
      "    Uninstalling numpy-1.24.0:\n",
      "      Successfully uninstalled numpy-1.24.0\n",
      "Successfully installed numpy-2.4.2 pandas-3.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40950cda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studia_Projekty/ComputerVision/image-classifier-cv-proj/.venv1/lib/python3.11/site-packages/pandas/__init__.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m     24\u001b[39m     )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studia_Projekty/ComputerVision/image-classifier-cv-proj/.venv1/lib/python3.11/site-packages/pandas/compat/__init__.py:27\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     CHAINED_WARNING_DISABLED,\n\u001b[32m     20\u001b[39m     IS64,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     WASM,\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     HAS_PYARROW,\n\u001b[32m     30\u001b[39m     PYARROW_MIN_VERSION,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     pa_version_under21p0,\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studia_Projekty/ComputerVision/image-classifier-cv-proj/.venv1/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py:18\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _nlv < Version(_min_numpy_ver):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     19\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to use this pandas version.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     24\u001b[39m np_long: \u001b[38;5;28mtype\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: Please upgrade numpy to >= 1.26.0 to use this pandas version.\nYour numpy version is 1.24.0.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# EXPERIMENT CONFIGURATION\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      7\u001b[39m ARCHITECTURES = {\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mVGG16\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m num_classes: VGG(get_vgg_layers(vgg16_config, batch_norm=\u001b[38;5;28;01mTrue\u001b[39;00m), num_classes),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mResNet18\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m num_classes: ResNetWrapper(num_classes, pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     10\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studia_Projekty/ComputerVision/image-classifier-cv-proj/.venv1/lib/python3.11/site-packages/pandas/__init__.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     26\u001b[39m     _module = _err.name\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     28\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not built. If you want to import \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpandas from the source directory, you may need to run \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto build the C extensions first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_err\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     get_option,\n\u001b[32m     36\u001b[39m     set_option,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     options,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "ARCHITECTURES = {\n",
    "    'VGG16': lambda num_classes: VGG(get_vgg_layers(vgg16_config, batch_norm=True), num_classes),\n",
    "    'ResNet18': lambda num_classes: ResNetWrapper(num_classes, pretrained=False),\n",
    "}\n",
    "\n",
    "# Experiment hyperparameters\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'num_classes': 5,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "}\n",
    "\n",
    "def run_single_experiment(arch_name, scenario, aug_variant, config=EXPERIMENT_CONFIG, verbose=True):\n",
    "    \"\"\"\n",
    "    Runs a single experiment and returns results.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Experiment: {arch_name} | {scenario} | {aug_variant}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader, class_names, sizes = create_data_loaders(\n",
    "        scenario, config['batch_size'], aug_variant\n",
    "    )\n",
    "    print(f\"Data sizes: train={sizes['train']}, val={sizes['val']}, test={sizes['test']}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = ARCHITECTURES[arch_name](config['num_classes'])\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model, history, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        config['num_epochs'], device, verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    metrics = evaluate_model(model, test_loader, class_names, device)\n",
    "    \n",
    "    print(f\"\\n--- Test Results ---\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"Precision: {metrics['precision']:.2f}%\")\n",
    "    print(f\"Recall: {metrics['recall']:.2f}%\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.2f}%\")\n",
    "    \n",
    "    # Package results\n",
    "    result = {\n",
    "        'architecture': arch_name,\n",
    "        'scenario': scenario,\n",
    "        'augmentation': aug_variant,\n",
    "        'train_size': sizes['train'],\n",
    "        'val_size': sizes['val'],\n",
    "        'test_size': sizes['test'],\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_accuracy': metrics['accuracy'],\n",
    "        'test_precision': metrics['precision'],\n",
    "        'test_recall': metrics['recall'],\n",
    "        'test_f1': metrics['f1_score'],\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'confusion_matrix': metrics['confusion_matrix'],\n",
    "        'class_names': class_names,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_all_experiments(architectures=None, scenarios=None, augmentations=None, config=EXPERIMENT_CONFIG):\n",
    "    \"\"\"\n",
    "    Runs all 32 experiments (or subset) and returns results DataFrame.\n",
    "    \"\"\"\n",
    "    if architectures is None:\n",
    "        architectures = list(ARCHITECTURES.keys())\n",
    "    if scenarios is None:\n",
    "        scenarios = list(DATA_SCENARIOS.keys())\n",
    "    if augmentations is None:\n",
    "        augmentations = list(AUGMENTATION_VARIANTS.keys())\n",
    "    \n",
    "    total = len(architectures) * len(scenarios) * len(augmentations)\n",
    "    print(f\"Running {total} experiments...\")\n",
    "    print(f\"Architectures: {architectures}\")\n",
    "    print(f\"Scenarios: {scenarios}\")\n",
    "    print(f\"Augmentations: {augmentations}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        for scenario in scenarios:\n",
    "            for aug in augmentations:\n",
    "                try:\n",
    "                    result = run_single_experiment(arch, scenario, aug, config, verbose=False)\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    # Save intermediate results\n",
    "                    model_name = f\"{arch}_{scenario}_{aug}\"\n",
    "                    torch.save(result['model'].state_dict(), f\"models/{model_name}.pth\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR in {arch}/{scenario}/{aug}: {e}\")\n",
    "                    all_results.append({\n",
    "                        'architecture': arch,\n",
    "                        'scenario': scenario,\n",
    "                        'augmentation': aug,\n",
    "                        'error': str(e),\n",
    "                    })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for r in all_results:\n",
    "        if 'error' not in r:\n",
    "            summary_data.append({\n",
    "                'Architecture': r['architecture'],\n",
    "                'Scenario': r['scenario'],\n",
    "                'Augmentation': r['augmentation'],\n",
    "                'Train Size': r['train_size'],\n",
    "                'Best Val Acc (%)': r['best_val_acc'],\n",
    "                'Test Acc (%)': r['test_accuracy'],\n",
    "                'Test Precision (%)': r['test_precision'],\n",
    "                'Test Recall (%)': r['test_recall'],\n",
    "                'Test F1 (%)': r['test_f1'],\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(summary_data)\n",
    "    return results_df, all_results\n",
    "\n",
    "print(\"Experiment runner configured.\")\n",
    "print(f\"Architectures: {list(ARCHITECTURES.keys())}\")\n",
    "print(f\"Scenarios: {list(DATA_SCENARIOS.keys())}\")\n",
    "print(f\"Augmentations: {list(AUGMENTATION_VARIANTS.keys())}\")\n",
    "print(f\"Total experiments: {len(ARCHITECTURES) * len(DATA_SCENARIOS) * len(AUGMENTATION_VARIANTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d0f53",
   "metadata": {},
   "source": [
    "## Run Single Experiment (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a150a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single experiment (VGG16, real data, baseline augmentation)\n",
    "test_result = run_single_experiment('VGG16', 'E3_real', 'A1_baseline', \n",
    "                                     config={**EXPERIMENT_CONFIG, 'num_epochs': 5})\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_result['confusion_matrix'], test_result['class_names'], \n",
    "                      title='VGG16 - E3_real - A1_baseline')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(test_result['history'], title='VGG16 - E3_real - A1_baseline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff6c29",
   "metadata": {},
   "source": [
    "## Run All 32 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments (this will take a while!)\n",
    "# Uncomment to run:\n",
    "\n",
    "# results_df, all_results = run_all_experiments()\n",
    "# results_df.to_csv('experiment_results.csv', index=False)\n",
    "# print(results_df.to_string())\n",
    "\n",
    "# Or run a subset:\n",
    "results_df, all_results = run_all_experiments(\n",
    "    architectures=['VGG16', 'ResNet18'],\n",
    "    scenarios=['E3_real'],  # Start with real data only\n",
    "    augmentations=['A1_baseline', 'A4_heavy'],\n",
    "    config={**EXPERIMENT_CONFIG, 'num_epochs': 10}\n",
    ")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce965e",
   "metadata": {},
   "source": [
    "## Results Visualization & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_charts(results_df):\n",
    "    \"\"\"\n",
    "    Creates comparison visualizations for experiment results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1. Architecture comparison\n",
    "    if len(results_df['Architecture'].unique()) > 1:\n",
    "        arch_comparison = results_df.groupby('Architecture')['Test Acc (%)'].mean()\n",
    "        arch_comparison.plot(kind='bar', ax=axes[0, 0], color=['steelblue', 'coral'])\n",
    "        axes[0, 0].set_title('Average Test Accuracy by Architecture')\n",
    "        axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=0)\n",
    "    \n",
    "    # 2. Data scenario comparison\n",
    "    if len(results_df['Scenario'].unique()) > 1:\n",
    "        scenario_comparison = results_df.groupby('Scenario')['Test Acc (%)'].mean()\n",
    "        scenario_comparison.plot(kind='bar', ax=axes[0, 1], color='teal')\n",
    "        axes[0, 1].set_title('Average Test Accuracy by Data Scenario')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. Augmentation comparison\n",
    "    if len(results_df['Augmentation'].unique()) > 1:\n",
    "        aug_comparison = results_df.groupby('Augmentation')['Test Acc (%)'].mean()\n",
    "        aug_comparison.plot(kind='bar', ax=axes[1, 0], color='purple')\n",
    "        axes[1, 0].set_title('Average Test Accuracy by Augmentation')\n",
    "        axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "        axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. Heatmap: Architecture x Scenario\n",
    "    if len(results_df['Architecture'].unique()) > 1 and len(results_df['Scenario'].unique()) > 1:\n",
    "        pivot = results_df.pivot_table(\n",
    "            values='Test Acc (%)', \n",
    "            index='Architecture', \n",
    "            columns='Scenario', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        im = axes[1, 1].imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "        axes[1, 1].set_xticks(range(len(pivot.columns)))\n",
    "        axes[1, 1].set_xticklabels(pivot.columns, rotation=45)\n",
    "        axes[1, 1].set_yticks(range(len(pivot.index)))\n",
    "        axes[1, 1].set_yticklabels(pivot.index)\n",
    "        axes[1, 1].set_title('Test Accuracy: Architecture x Scenario')\n",
    "        plt.colorbar(im, ax=axes[1, 1])\n",
    "        \n",
    "        # Annotate cells\n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                axes[1, 1].text(j, i, f'{pivot.values[i, j]:.1f}%', \n",
    "                               ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_all_confusion_matrices(all_results, max_cols=4):\n",
    "    \"\"\"\n",
    "    Plots confusion matrices for all experiments in a grid.\n",
    "    \"\"\"\n",
    "    valid_results = [r for r in all_results if 'error' not in r]\n",
    "    n = len(valid_results)\n",
    "    if n == 0:\n",
    "        print(\"No valid results to plot.\")\n",
    "        return\n",
    "    \n",
    "    n_cols = min(max_cols, n)\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    if n == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif n_cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for idx, result in enumerate(valid_results):\n",
    "        row, col = idx // n_cols, idx % n_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        cm = result['confusion_matrix']\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=result['class_names'])\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=False)\n",
    "        ax.set_title(f\"{result['architecture']}\\n{result['scenario']}\\n{result['augmentation']}\", fontsize=9)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n, n_rows * n_cols):\n",
    "        row, col = idx // n_cols, idx % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot comparisons if results exist\n",
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    plot_comparison_charts(results_df)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_all_confusion_matrices(all_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077d6be",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10857c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    results_df.to_csv('experiment_results.csv', index=False)\n",
    "    print(\"Results saved to experiment_results.csv\")\n",
    "    print(\"\\nFinal Results Table:\")\n",
    "    print(results_df.to_string())\n",
    "\n",
    "# Save best model\n",
    "if 'all_results' in dir() and len(all_results) > 0:\n",
    "    valid_results = [r for r in all_results if 'error' not in r and 'test_accuracy' in r]\n",
    "    if valid_results:\n",
    "        best = max(valid_results, key=lambda x: x['test_accuracy'])\n",
    "        best_name = f\"best_{best['architecture']}_{best['scenario']}_{best['augmentation']}.pth\"\n",
    "        torch.save(best['model'].state_dict(), f\"models/{best_name}\")\n",
    "        print(f\"\\nBest model saved: {best_name}\")\n",
    "        print(f\"  Test Accuracy: {best['test_accuracy']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
