{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a11fc11",
   "metadata": {},
   "source": [
    "# VGG classifier training and fine tunning comparison\n",
    "\n",
    "In this notebook I want to compare the accuracy of a VGG model build from scratch and trained on the target dataset (bike category recognition) with a pre trained model (ImageNet) and fine tunned model.\n",
    "\n",
    "I'll be using discriminative fine-tunning in this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ce4c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  Detected classes: ['road', 'gravel', 'fold', 'mtb', 'hybrid']\n",
      "üîÑ Processing: road...\n",
      "üîÑ Processing: gravel...\n",
      "üîÑ Processing: fold...\n",
      "üîÑ Processing: mtb...\n",
      "üîÑ Processing: hybrid...\n",
      "\n",
      "‚úÖ Done! Dataset organized at: dataset_split\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils.load_dataset import load_dataset\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "dataset_url = os.getenv(\"DATASET_REPO_URL\")\n",
    "\n",
    "load_dataset(dataset_url, extract_dir=\"dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9d1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8178dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd9a0",
   "metadata": {},
   "source": [
    "## Architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2765edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8df98",
   "metadata": {},
   "source": [
    "### VGG configuration arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92676af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512,\n",
    "                512, 'M']\n",
    "\n",
    "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512,\n",
    "                'M', 512, 512, 512, 'M']\n",
    "\n",
    "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512,\n",
    "                512, 512, 'M', 512, 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e453f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_layers(config, batch_norm):\n",
    "\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        if c == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = c\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80407639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU(inplace=True)\n",
      "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n",
    "\n",
    "vgg16_layers = get_vgg_layers(vgg16_config, batch_norm=True)\n",
    "\n",
    "print(vgg11_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8456b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def data_loader(data_dir, batch_size, set_type='train', shuffle=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # sprawdzic dodanie normalizacji\n",
    "    ])\n",
    "\n",
    "    set_path = os.path.join(data_dir, set_type)\n",
    "\n",
    "    # ImageFolder\n",
    "    dataset = datasets.ImageFolder(set_path, transform=transform)\n",
    "\n",
    "    # DataLoader\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=shuffle)\n",
    "\n",
    "    size = len(dataset)\n",
    "    return loader, size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af81e3d",
   "metadata": {},
   "source": [
    "## Define train set augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9390b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms = transforms.Compose([\n",
    "# \t\t\ttransforms.RandomRotation(5),\n",
    "# \t\t\ttransforms.RandomHorizontalFlip(0.5),\n",
    "# \t\t\ttransforms.RandomCrop(32, padding=2),\n",
    "# \t\t\ttransforms.ToTensor(),\n",
    "# \t\t\ttransforms.Normalize(mean=means,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\tstd=stds)\n",
    "# \t])\n",
    "\n",
    "# test_transforms = transforms.Compose([\n",
    "# \t\t\ttransforms.ToTensor(),\n",
    "# \t\t\ttransforms.Normalize(mean=means,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\tstd=stds)\n",
    "# \t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f136a1",
   "metadata": {},
   "source": [
    "## VGG Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24ab7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = VGG(vgg11_layers, num_classes)\n",
    "\n",
    "model16 = VGG(vgg16_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "optimizer16 = torch.optim.Adam(model16.parameters(), lr=learning_rate, weight_decay = weight_decay)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9ec43",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01a1d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs):\n",
    "\ttotal_step = len(train_loader)\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tfor images, labels in train_loader:  \n",
    "\t\t\t# Move tensors to the configured device\n",
    "\t\t\timages = images.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# Forward pass\n",
    "\t\t\toutputs, h = model(images)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\t\n",
    "\t\t\t# Backward and optimize\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\tprint ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "\t\t\t\t\t\t\t\t\t\t.format(epoch+1, num_epochs, epoch+1, total_step, loss.item()))\n",
    "\t\t\t\t\t\t\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tcorrect = 0\n",
    "\t\t\ttotal = 0\n",
    "\t\t\t\n",
    "\t\t\tfor images, labels in valid_loader:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\t\t\t\toutputs, h = model(images)\n",
    "\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\t\t\ttotal += labels.size(0)\n",
    "\t\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\t\t\tdel images, labels, outputs\n",
    "\t\n",
    "\t\t\tprint('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f428a2",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40950cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPLIT_PATH = 'dataset_split/real'\n",
    "\n",
    "train_loader, train_size = data_loader(DATASET_SPLIT_PATH, batch_size, set_type='train', shuffle=True)\n",
    "valid_loader, valid_size = data_loader(DATASET_SPLIT_PATH, batch_size, set_type='val', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "031d0f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1/2], Loss: 1.7130\n",
      "Accuracy of the network on the 5000 validation images: 33.333333333333336 %\n",
      "Epoch [2/50], Step [2/2], Loss: 1.0697\n",
      "Accuracy of the network on the 5000 validation images: 40.0 %\n",
      "Epoch [3/50], Step [3/2], Loss: 0.9825\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [4/50], Step [4/2], Loss: 1.3874\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [5/50], Step [5/2], Loss: 0.6614\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [6/50], Step [6/2], Loss: 0.9899\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [7/50], Step [7/2], Loss: 0.6997\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [8/50], Step [8/2], Loss: 0.4332\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [9/50], Step [9/2], Loss: 0.5098\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [10/50], Step [10/2], Loss: 0.7741\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [11/50], Step [11/2], Loss: 0.2477\n",
      "Accuracy of the network on the 5000 validation images: 40.0 %\n",
      "Epoch [12/50], Step [12/2], Loss: 0.6170\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [13/50], Step [13/2], Loss: 0.7523\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [14/50], Step [14/2], Loss: 0.3791\n",
      "Accuracy of the network on the 5000 validation images: 40.0 %\n",
      "Epoch [15/50], Step [15/2], Loss: 0.3997\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [16/50], Step [16/2], Loss: 0.7660\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [17/50], Step [17/2], Loss: 0.5842\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [18/50], Step [18/2], Loss: 0.9090\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [19/50], Step [19/2], Loss: 0.5967\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [20/50], Step [20/2], Loss: 0.3094\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [21/50], Step [21/2], Loss: 0.6912\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [22/50], Step [22/2], Loss: 0.2972\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [23/50], Step [23/2], Loss: 0.3211\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [24/50], Step [24/2], Loss: 0.4215\n",
      "Accuracy of the network on the 5000 validation images: 73.33333333333333 %\n",
      "Epoch [25/50], Step [25/2], Loss: 0.3434\n",
      "Accuracy of the network on the 5000 validation images: 80.0 %\n",
      "Epoch [26/50], Step [26/2], Loss: 0.2585\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [27/50], Step [27/2], Loss: 0.8807\n",
      "Accuracy of the network on the 5000 validation images: 73.33333333333333 %\n",
      "Epoch [28/50], Step [28/2], Loss: 0.4578\n",
      "Accuracy of the network on the 5000 validation images: 73.33333333333333 %\n",
      "Epoch [29/50], Step [29/2], Loss: 0.1856\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [30/50], Step [30/2], Loss: 0.3243\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [31/50], Step [31/2], Loss: 0.0932\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [32/50], Step [32/2], Loss: 0.8591\n",
      "Accuracy of the network on the 5000 validation images: 80.0 %\n",
      "Epoch [33/50], Step [33/2], Loss: 0.0882\n",
      "Accuracy of the network on the 5000 validation images: 73.33333333333333 %\n",
      "Epoch [34/50], Step [34/2], Loss: 2.3160\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [35/50], Step [35/2], Loss: 0.3291\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [36/50], Step [36/2], Loss: 0.3744\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [37/50], Step [37/2], Loss: 0.2037\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [38/50], Step [38/2], Loss: 0.2086\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [39/50], Step [39/2], Loss: 0.2520\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [40/50], Step [40/2], Loss: 0.0364\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "Epoch [41/50], Step [41/2], Loss: 1.0408\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [42/50], Step [42/2], Loss: 0.5628\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [43/50], Step [43/2], Loss: 0.0368\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [44/50], Step [44/2], Loss: 0.4857\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [45/50], Step [45/2], Loss: 0.1754\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [46/50], Step [46/2], Loss: 0.3228\n",
      "Accuracy of the network on the 5000 validation images: 53.333333333333336 %\n",
      "Epoch [47/50], Step [47/2], Loss: 0.2126\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [48/50], Step [48/2], Loss: 0.1154\n",
      "Accuracy of the network on the 5000 validation images: 66.66666666666667 %\n",
      "Epoch [49/50], Step [49/2], Loss: 0.3468\n",
      "Accuracy of the network on the 5000 validation images: 60.0 %\n",
      "Epoch [50/50], Step [50/2], Loss: 0.3407\n",
      "Accuracy of the network on the 5000 validation images: 46.666666666666664 %\n",
      "------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# result = train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "result = train_model(model16, train_loader, valid_loader, criterion, optimizer16, num_epochs)\n",
    "print(\"-\" * 30)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to vgg16_bike_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'models'\n",
    "\n",
    "torch.save(model16.state_dict(), os.path.join(MODEL_DIR, 'vgg16_bike_classifier.pth'))\n",
    "print(\"Model saved to vgg16_bike_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
